name: "LLM API Service Implementation"
description: |

## Purpose

Implement a generic LLM API service that abstracts LLM provider interactions, supporting multiple providers (OpenAI, Anthropic, etc.) with proper error handling and configuration management.

## Core Principles

1. **Context is Complete but Focused**: Include ALL necessary documentation sections, specific examples, and discovered caveats by linking specific documents
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md
6. **Condense Repeated Code**: Refactor code that repeated

---

## Goal

Create an `LlmApi` service that provides a unified interface for generating text completions from various LLM providers (OpenAI, Anthropic, Google, etc.). The service should:
- Abstract provider-specific implementation details
- Handle authentication via API keys from configuration
- Provide proper error handling with `LlmApiError`
- Support multiple providers configured through environment variables
- Be easily testable with mock implementations

## Why

- **Business value**: Enables the core test execution functionality by allowing prompts to be sent to LLMs
- **Integration**: Critical dependency for `TestExecutionService` (Phase 6.3)
- **Problems solved**: Abstracts away provider differences, handles retries, manages API keys securely

## What

A service that exposes a simple `generate(systemPrompt: string, prompt: string, model: string): Effect<string, LlmApiError>` method that:
- Accepts a system prompt to define the assistant's behavior
- Accepts a user prompt for the actual query
- Automatically selects the correct provider based on the model name
- Makes HTTP requests to the provider's API
- Handles common errors (rate limits, timeouts, invalid API keys)
- Returns the generated text response

### Success Criteria

- [ ] Service can successfully call OpenAI API with proper authentication
- [ ] Service can successfully call Anthropic API with proper authentication
- [ ] Errors are properly typed as `LlmApiError` with provider and status code
- [ ] Configuration is read from environment variables via ConfigService
- [ ] Test layer allows mocking LLM responses with data-driven approach
- [ ] Integration test demonstrates real API call (skipped by default in tests)

## All Needed Context

### Documentation & References (include complete sections that are directly relevant)

```yaml
# MUST READ - Include these specific sections in your context window
# ✅ Include: Complete relevant sections, not just snippets
# ❌ Avoid: Entire folders or unrelated documentation

- url: https://platform.openai.com/docs/api-reference/chat/create
  sections: ['Authentication', 'Create chat completion', 'The chat completion object']
  why: OpenAI is a primary provider, need exact request/response format
  discovered_caveat: Uses Bearer token authentication, max tokens defaults to infinity

- url: https://docs.anthropic.com/en/api/messages
  sections: ['Authentication', 'Create a Message', 'The message object']
  why: Anthropic is another primary provider
  discovered_caveat: Uses x-api-key header, requires anthropic-version header

- file: examples/effect-official-examples/examples/http-server/src/client.ts
  why: Shows Effect HTTP client usage pattern
  gotcha: Must use HttpClient.filterStatusOk to handle non-200 responses

- file: docs/llms/guides/effect-packages/platform-node/examples/http-client.ts
  why: Complete example of HTTP client service with Effect
  critical: Shows proper error handling and request transformation patterns

- docfile: docs/llms/guides/effect-docs/
  sections: ['Services', 'Layers', 'Error handling']
  why: Core Effect patterns for service implementation

- file: src/services/neo4j/Neo4j.service.ts
  why: Follow this exact pattern for service definition
  pattern: Interface + Context.Tag class pattern

- file: src/layers/configuration/Configuration.layer.ts
  why: Shows how to read configuration and create layers
  pattern: Effect.gen with Config module usage

- file: src/domain/types/errors.ts
  why: LlmApiError is already defined here
  note: Has provider, statusCode, originalMessage fields

- url: https://www.npmjs.com/package/@effect/platform
  sections: ['HttpClient', 'HttpClientRequest', 'HttpClientResponse']
  why: HTTP client implementation details

- url: https://www.npmjs.com/package/@effect/platform-node
  sections: ['NodeHttpClient']
  why: Node.js specific HTTP client layer
```

### Context Inclusion Guidelines

- Include COMPLETE sections when they contain implementation details
- Include MULTIPLE examples if they show different use cases
- Include ALL caveats and warnings discovered during research
- Skip sections about: history, philosophy, future plans, unrelated features
- When in doubt, include it - but be specific about WHY it's needed

### Current Codebase tree (run `tree` in the root of the project) to get an overview of the codebase

```bash
src/
├── domain/
│   ├── index.ts
│   └── types/
│       ├── branded.ts
│       ├── composition.ts
│       ├── config.ts
│       ├── database.ts
│       ├── errors.ts         # LlmApiError already defined here
│       ├── experiment.ts
│       ├── index.ts
│       ├── parameter.ts
│       ├── snippet.ts
│       ├── tag.ts
│       └── tests/
├── layers/
│   ├── configuration/
│   │   ├── Configuration.layer.ts
│   │   ├── Configuration.test-layers.ts
│   │   ├── Configuration.test.ts
│   │   └── index.ts
│   ├── index.ts
│   └── neo4j/
│       ├── Neo4j.layer.ts
│       └── index.ts
├── lib/
│   └── test-utils.ts
└── services/
    ├── config/
    │   ├── index.ts         # ConfigService definition
    │   └── utils.ts
    ├── neo4j/
    │   ├── Neo4j.service.ts # Service pattern example
    │   ├── Neo4j.test.ts
    │   └── index.ts
    ├── persistence/
    │   ├── GenericPersistence.test-layers.ts
    │   ├── GenericPersistence.test.ts
    │   ├── GenericPersistence.ts
    │   └── index.ts
    └── snippet/
        ├── SnippetPersistence.test-layers.ts
        ├── SnippetPersistence.test.ts
        ├── SnippetPersistence.ts
        └── index.ts
```

### Desired Codebase tree with files to be added and responsibility of file

```bash
src/
└── services/
    └── llm-api/
        ├── index.ts              # Service definition (LlmApiService tag)
        ├── LlmApi.service.ts     # Service interface definition
        └── LlmApi.test.ts        # Service tests
└── layers/
    └── llm-api/
        ├── LlmApi.layer.ts       # Live implementation with HTTP calls
        ├── LlmApi.test-layers.ts # Test layer with mocked responses
        ├── LlmApi.test.ts        # Tests for the layers themselves
        └── index.ts              # Re-exports
```

### Domain Structure & Naming Conventions

```
src/
├── domain/
│   └── types/              # Domain types and schemas
│       ├── <type>.ts       # Type definitions using Schema.Struct
│       └── tests/
│           └── <type>.test.ts
├── services/
│   └── <service-name>/
│       └── index.ts
└── layers/
    └── <layer-domain>/
        ├── <LayerName>.layer.ts         # Live/production implementation
        ├── <LayerName>.staticLayers.ts  # Test implementation with static data
        ├── <LayerName>.test.ts          # Tests for the layers
        └── index.ts                     # Re-exports
```

**Key Naming Conventions:**

- `*.layer.ts` - Production layers that may have side effects
- `*.staticLayers.ts` - Test layers with hardcoded static data (no side effects)
- `*.test.ts` - Test files that test the layers

### Known Gotchas of our codebase & Library Quirks

```typescript
// CRITICAL: Effect.gen requires function* syntax for generators
// CRITICAL: Schema.decode returns an Effect, not a plain value
// CRITICAL: We use Effect v3 and Schema.Struct for Neo4j (not Model.Class)
// CRITICAL: Always use Config.redacted for API keys and secrets
// CRITICAL: HttpClient requires explicit error handling with filterStatusOk
// CRITICAL: Use @effect/platform for HTTP, not native fetch
// CRITICAL: Test layers should use makeTestLayerFor utility when possible
```

## Implementation Blueprint

### Data models and structure

Create the core data models, we ensure type safety and consistency.

```typescript
Examples:
 - LlmApiError already exists in src/domain/types/errors.ts
 - ConfigService already provides llm.providers with apiKey, baseUrl, model
 - Use HttpClient from @effect/platform for requests
 - Service interface pattern from Neo4jService
```

### list of tasks to be completed to fullfill the PRP in the order they should be completed

```yaml
Task 1: Create branded types for LLM domain
MODIFY src/domain/types/branded.ts:
  - Add SystemPrompt branded type (NonEmptyString with max length)
  - Add UserPrompt branded type (NonEmptyString with max length)
  - Add to AnyId union if needed

Task 2: Create LlmApi service definition
CREATE src/services/llm-api/index.ts:
  - MIRROR pattern from: src/services/neo4j/index.ts
  - Export LlmApiService class extending Context.Tag
  - Re-export from LlmApi.service.ts

CREATE src/services/llm-api/LlmApi.service.ts:
  - MIRROR pattern from: src/services/neo4j/Neo4j.service.ts
  - Define LlmApiImpl interface with generate method using branded types
  - Create LlmApiService Context.Tag

Task 3: Create LlmApi layer implementation
CREATE src/layers/llm-api/index.ts:
  - Export LlmApiLive and LlmApiTest

CREATE src/layers/llm-api/LlmApi.layer.ts:
  - PATTERN from: docs/llms/guides/effect-packages/platform-node/examples/http-client.ts
  - PATTERN from: src/layers/configuration/Configuration.layer.ts for config access
  - Implement provider selection logic based on model prefix
  - Implement HTTP calls to OpenAI and Anthropic
  - Handle errors with proper LlmApiError construction

Task 4: Create test layer
CREATE src/layers/llm-api/LlmApi.test-layers.ts:
  - PATTERN from: src/layers/neo4j/Neo4j.layer.ts (Neo4jTest implementation)
  - Use makeTestLayerFor utility from src/lib/test-utils.ts
  - Provide mock responses for different prompts

Task 5: Create comprehensive tests
CREATE src/services/llm-api/LlmApi.test.ts:
  - PATTERN from: src/services/neo4j/Neo4j.test.ts
  - Test successful generation
  - Test error handling (invalid API key, network error, rate limit)
  - Test provider selection based on model name
  - Test with both live and test layers

Task 6: Create layer tests
CREATE src/layers/llm-api/LlmApi.test.ts:
  - PATTERN from: src/layers/configuration/Configuration.test.ts
  - Test LlmApiLive layer with mocked config and HTTP client
  - Test provider selection logic
  - Test request building for different providers
  - Test error handling scenarios
  - Test with ConfigProvider.fromMap for mocked config

Task 7: Update exports
MODIFY src/services/index.ts:
  - Add export for LlmApiService

MODIFY src/layers/index.ts:
  - Add exports for LlmApiLive and LlmApiTest

Task 8: Integration test and documentation
CREATE src/services/llm-api/LlmApi.integration.test.ts:
  - Use describe.skipIf pattern to skip entire suite
  - Check for INTEGRATION_TEST env var or missing API keys
  - Test real API call with environment variables
  - Ensure this test is NOT run by default in pnpm test
  
UPDATE README.md:
  - Add LLM provider configuration section
  - Document required environment variables
```

### Per task pseudocode as needed added to each task

```typescript
// Task 1 - Branded Types
// src/domain/types/branded.ts
export const SystemPrompt = Schema.String.pipe(
  Schema.nonEmptyString({
    message: () => 'System prompt cannot be empty',
  }),
  Schema.maxLength(2000, {
    message: () => 'System prompt must be 2000 characters or less',
  }),
  Schema.brand('SystemPrompt'),
);
export type SystemPrompt = typeof SystemPrompt.Type;

export const UserPrompt = Schema.String.pipe(
  Schema.nonEmptyString({
    message: () => 'User prompt cannot be empty',
  }),
  Schema.maxLength(10000, {
    message: () => 'User prompt must be 10000 characters or less',
  }),
  Schema.brand('UserPrompt'),
);
export type UserPrompt = typeof UserPrompt.Type;

// Task 2 - Service Definition
// src/services/llm-api/LlmApi.service.ts
import { SystemPrompt, UserPrompt } from '../../domain/types/branded';
import { LlmModel } from '../../domain/types/database';

export interface LlmApiImpl {
  readonly generate: (
    systemPrompt: SystemPrompt,
    prompt: UserPrompt,
    model: LlmModel,
  ) => Effect.Effect<string, LlmApiError, never>;
}

export class LlmApiService extends Context.Tag('LlmApiService')<
  LlmApiService,
  LlmApiImpl
>() {}

// Task 3 - Layer Implementation
// src/layers/llm-api/LlmApi.layer.ts
const makeService = Effect.gen(function* () {
  const config = yield* ConfigService;
  const httpClient = yield* HttpClient.HttpClient;
  
  // Determine provider from model name
  const getProvider = (model: LlmModel): Effect.Effect<ProviderName, LlmApiError> => {
    const modelStr = model as string;
    if (modelStr.startsWith('gpt-')) {
      return Effect.succeed(Schema.decodeSync(ProviderName)('openai'));
    }
    if (modelStr.startsWith('claude-')) {
      return Effect.succeed(Schema.decodeSync(ProviderName)('anthropic'));
    }
    // Add more providers as needed
    return Effect.fail(new LlmApiError({
      provider: 'unknown',
      originalMessage: `Unknown model: ${modelStr}`,
    }));
  };
  
  const generate = (systemPrompt: SystemPrompt, prompt: UserPrompt, model: LlmModel) =>
    Effect.gen(function* () {
      const provider = yield* getProvider(model);
      const providerConfig = config.llm.providers[provider];
      
      if (!providerConfig) {
        return yield* Effect.fail(new LlmApiError({
          provider,
          originalMessage: `Provider ${provider} not configured`,
        }));
      }
      
      // Build request based on provider
      const request = provider === 'openai' 
        ? buildOpenAIRequest(systemPrompt, prompt, model, providerConfig)
        : buildAnthropicRequest(systemPrompt, prompt, model, providerConfig);
      
      // Execute with error handling
      const response = yield* pipe(
        httpClient.execute(request),
        HttpClient.filterStatusOk,
        Effect.mapError(error => new LlmApiError({
          provider,
          statusCode: error.response?.status,
          originalMessage: error.message,
        })),
      );
      
      // Parse response based on provider
      const body = yield* response.json;
      return extractContent(body, provider);
    });
    
  return { generate };
});

// Task 4 - Test Layer with data-driven approach
// src/layers/llm-api/LlmApi.test-layers.ts
import { SystemPrompt, UserPrompt } from '../../domain/types/branded';
import { LlmModel } from '../../domain/types/database';

// Test data structure
export interface LlmApiTestData {
  responses: Map<string, { 
    systemPrompt: SystemPrompt; 
    prompt: UserPrompt; 
    model: LlmModel; 
    response: string | Error 
  }>;
}

// Helper to create test data entries
const makeTestEntry = (
  systemPrompt: string,
  prompt: string, 
  model: string,
  response: string | Error
) => ({
  systemPrompt: Schema.decodeSync(SystemPrompt)(systemPrompt),
  prompt: Schema.decodeSync(UserPrompt)(prompt),
  model: Schema.decodeSync(LlmModel)(model),
  response,
});

// Default test data
export const defaultLlmApiTestData: LlmApiTestData = {
  responses: new Map([
    ['test-success', makeTestEntry(
      'You are a helpful assistant',
      'Hello world',
      'gpt-4',
      'Generated response for: Hello world',
    )],
    ['test-error', makeTestEntry(
      'You are a helpful assistant',
      'trigger error',
      'gpt-4',
      new LlmApiError({
        provider: 'openai',
        statusCode: 500,
        originalMessage: 'Internal server error',
      }),
    )],
  ]),
};

export const LlmApiTest = (testData: LlmApiTestData = defaultLlmApiTestData) => {
  const mockService = LlmApiService.of({
    generate: (systemPrompt, prompt, model) => {
      // Find matching test case
      for (const [key, data] of testData.responses) {
        if (data.prompt === prompt && data.model === model) {
          if (data.response instanceof Error) {
            return Effect.fail(data.response);
          }
          return Effect.succeed(data.response);
        }
      }
      // Default response
      return Effect.succeed(`Generated: ${systemPrompt} | ${prompt} | ${model}`);
    },
  });
  
  return Layer.succeed(LlmApiService, mockService);
};

// Task 6 - Layer Tests
// src/layers/llm-api/LlmApi.test.ts
import { describe, it } from '@effect/vitest';
import { Effect, ConfigProvider, Layer } from 'effect';
import { HttpClient } from '@effect/platform';
import { LlmApiService } from '../../services/llm-api';
import { LlmApiLive } from './LlmApi.layer';
import { ConfigServiceLive } from '../configuration';

describe('LlmApiLive', () => {
  it.effect('should make correct OpenAI API request', () =>
    Effect.gen(function* () {
      const mockConfig = {
        NEO4J_URI: 'bolt://localhost:7687',
        NEO4J_USER: 'neo4j',
        NEO4J_PASSWORD: 'test',
        LLM_PROVIDERS: 'openai',
        LLM_OPENAI_API_KEY: 'test-key',
        LLM_OPENAI_BASE_URL: 'https://api.openai.com/v1',
        LLM_OPENAI_MODEL: 'gpt-4',
      };
      
      // Mock HTTP client that captures requests
      const capturedRequests: any[] = [];
      const mockHttpClient = HttpClient.HttpClient.of({
        execute: (request) => {
          capturedRequests.push(request);
          return Effect.succeed({
            status: 200,
            json: Effect.succeed({
              choices: [{ message: { content: 'Test response' } }],
            }),
          });
        },
      });
      
      const llmApi = yield* LlmApiService.pipe(
        Effect.provide(LlmApiLive),
        Effect.provide(ConfigServiceLive),
        Effect.provide(Layer.succeed(HttpClient.HttpClient, mockHttpClient)),
        Effect.withConfigProvider(
          ConfigProvider.fromMap(new Map(Object.entries(mockConfig)))
        ),
      );
      
      const systemPrompt = yield* Schema.decode(SystemPrompt)('Be helpful');
      const userPrompt = yield* Schema.decode(UserPrompt)('Hello');
      const model = yield* Schema.decode(LlmModel)('gpt-4');
      
      const result = yield* llmApi.generate(systemPrompt, userPrompt, model);
      
      expect(result).toBe('Test response');
      expect(capturedRequests).toHaveLength(1);
      expect(capturedRequests[0].url).toContain('api.openai.com');
    }),
  );
});
```

### Integration Points

```yaml
CONFIG:
  - reads from: ConfigService
  - uses: config.llm.providers[providerName].apiKey/baseUrl/model
  - pattern: "const config = yield* ConfigService"

HTTP:
  - dependency: @effect/platform HttpClient
  - layer: NodeHttpClient from @effect/platform-node
  - pattern: "HttpClient.filterStatusOk"

ERRORS:
  - type: LlmApiError from src/domain/types/errors.ts
  - fields: provider, statusCode, originalMessage
  - pattern: "Effect.mapError(e => new LlmApiError({...}))"
```

## Validation Loop

### Level 1: Syntax & Type Checking

```bash
# Run these FIRST - fix any errors before proceeding
pnpm run build                    # TypeScript compilation
pnpm run lint                     # ESLint checking

# Expected: No errors. If errors, READ the error and fix.
```

### Level 2: Unit Tests each new feature/file/function use existing test patterns

```typescript
// CREATE src/services/llm-api/LlmApi.test.ts with these test cases:
import { it } from '@effect/vitest';
import { Effect, Exit, Schema } from 'effect';
import { LlmApiService } from './index';
import { LlmApiTest, defaultLlmApiTestData } from '../../layers/llm-api';
import { LlmApiError } from '../../domain/types/errors';
import { SystemPrompt, UserPrompt } from '../../domain/types/branded';
import { LlmModel } from '../../domain/types/database';

it.effect('should generate text successfully', () =>
  Effect.gen(function* () {
    const llmApi = yield* LlmApiService;
    
    // Decode inputs to branded types
    const systemPrompt = yield* Schema.decode(SystemPrompt)('You are a helpful assistant');
    const userPrompt = yield* Schema.decode(UserPrompt)('Hello world');
    const model = yield* Schema.decode(LlmModel)('gpt-4');
    
    const result = yield* llmApi.generate(systemPrompt, userPrompt, model);
    expect(result).toContain('Generated response');
  }).pipe(Effect.provide(LlmApiTest())),
);

it.effect('should handle API errors', () =>
  Effect.gen(function* () {
    const llmApi = yield* LlmApiService;
    const exit = yield* Effect.exit(
      llmApi.generate(
        'You are a helpful assistant',
        'trigger error',
        'gpt-4'
      )
    );
    expect(Exit.isFailure(exit)).toBe(true);
    if (exit._tag === 'Failure') {
      const error = exit.cause._tag === 'Fail' ? exit.cause.error : null;
      expect(error).toBeInstanceOf(LlmApiError);
      expect(error?.provider).toBe('openai');
      expect(error?.statusCode).toBe(500);
    }
  }).pipe(Effect.provide(LlmApiTest())),
);

it.effect('should select correct provider based on model', () =>
  Effect.gen(function* () {
    const llmApi = yield* LlmApiService;
    // Test OpenAI model
    const gptResult = yield* llmApi.generate(
      'System prompt',
      'test prompt',
      'gpt-4'
    );
    expect(gptResult).toContain('gpt-4');
    
    // Test Anthropic model  
    const claudeResult = yield* llmApi.generate(
      'System prompt',
      'test prompt',
      'claude-3-opus'
    );
    expect(claudeResult).toContain('claude-3-opus');
  }).pipe(Effect.provide(LlmApiTest())),
);

it.effect('should use custom test data', () =>
  Effect.gen(function* () {
    const customTestData = {
      responses: new Map([
        ['custom', {
          systemPrompt: 'Be creative',
          prompt: 'Write a poem',
          model: 'gpt-3.5-turbo',
          response: 'Roses are red, violets are blue',
        }],
      ]),
    };
    
    const llmApi = yield* LlmApiService;
    const result = yield* llmApi.generate(
      'Be creative',
      'Write a poem',
      'gpt-3.5-turbo'
    );
    expect(result).toBe('Roses are red, violets are blue');
  }).pipe(Effect.provide(LlmApiTest(customTestData))),
);
```

```bash
# Run and iterate until passing:
pnpm test src/services/llm-api/LlmApi.test.ts
# If failing: Read error, understand root cause, fix code, re-run
```

### Level 3: Integration Test

```typescript
// src/services/llm-api/LlmApi.integration.test.ts
import { describe, it } from '@effect/vitest';
import { Effect } from 'effect';
import { LlmApiService } from './index';
import { LlmApiLive } from '../../layers/llm-api';
import { ConfigServiceLive } from '../../layers/configuration';
import { NodeHttpClient } from '@effect/platform-node';

// Skip unless explicitly running integration tests
const shouldRunIntegration = process.env.INTEGRATION_TEST === 'true' || 
                            process.env.RUN_INTEGRATION_TESTS === 'true';

describe.skipIf(!shouldRunIntegration)('LlmApi Integration Tests', () => {
  it.effect('should make real API call to OpenAI', () =>
    Effect.gen(function* () {
      const llmApi = yield* LlmApiService;
      const result = yield* llmApi.generate(
        'You are a helpful assistant. Respond in one sentence.',
        'What is 2+2?',
        'gpt-3.5-turbo'
      );
      
      expect(result).toBeTruthy();
      expect(result.length).toBeGreaterThan(0);
      // Should contain something about "4" or "four"
    }).pipe(
      Effect.provide(LlmApiLive),
      Effect.provide(ConfigServiceLive),
      Effect.provide(NodeHttpClient.layer)
    ),
    { timeout: 30000 } // 30 second timeout for API calls
  );
});
```

```bash
# Run integration tests explicitly
INTEGRATION_TEST=true pnpm test src/services/llm-api/LlmApi.integration.test.ts

# Or set up environment and run
export LLM_OPENAI_API_KEY=your-key
export LLM_OPENAI_BASE_URL=https://api.openai.com/v1
export LLM_OPENAI_MODEL=gpt-3.5-turbo
INTEGRATION_TEST=true pnpm test

# Expected: Real API call succeeds with actual response
# NOTE: This will NOT run during normal `pnpm test` or `pnpm preflight`
```

## Final validation Checklist

- [ ] All tests pass: `pnpm test`
- [ ] No linting errors: `pnpm run lint`
- [ ] No type errors: `pnpm run build`
- [ ] Preflight passes: `pnpm run preflight`
- [ ] Integration test successful (if API keys available)
- [ ] Error cases handled with proper LlmApiError types
- [ ] Multiple providers supported (OpenAI, Anthropic)
- [ ] API keys properly redacted in logs
- [ ] Documentation updated with configuration instructions

---

## Anti-Patterns to Avoid

- ❌ Don't hardcode API keys - use ConfigService
- ❌ Don't use native fetch - use Effect HttpClient
- ❌ Don't catch all errors - use typed LlmApiError
- ❌ Don't use try/catch - use Effect error handling
- ❌ Don't expose raw HTTP responses - return only generated text
- ❌ Don't forget to handle rate limits and retries
- ❌ Don't mix provider logic - keep them separate
- ❌ Don't mutate configuration - keep everything immutable