name: "LLM API Service Implementation"
description: |

## Purpose

Implement an `LlmApi` service that abstracts LLM provider interactions, enabling the Janus project to call different LLM providers (OpenAI, Anthropic, Google, etc.) through a unified interface using the `@effect/ai` package.

## Core Principles

1. **Context is Complete but Focused**: Include ALL necessary documentation sections, specific examples, and discovered caveats by linking specific documents
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance
5. **Global rules**: Be sure to follow all rules in CLAUDE.md
6. **Condense Repeated Code**: Refactor code that repeated

---

## Goal

Create an `LlmApi` service that:
- Abstracts LLM provider interactions using `@effect/ai` package
- Exposes `generate(conversation: Conversation, model: string): Effect<string, LlmApiError>` method
- Creates `LlmApiLive` layer that reads provider configurations from `Config` service
- Automatically detects available providers based on which ones are configured in environment
- Uses the model specified in the method call (not from environment variables)
- Includes integration tests that make real API calls when `INTEGRATION_TEST=true`

## Why

- **Business value**: Enables the test execution system to run conversations against different LLM models
- **Integration**: Essential for Phase 6 - Test Execution & Results functionality
- **Problems solved**: Provides a unified interface for multiple LLM providers, making it easy to switch providers or run A/B tests

## What

Service that takes a conversation (using the existing `Conversation` type which is `Chunk<Message>` where `Message` has `role` and `content` fields) and a model string, then returns the LLM's response as a string. The service automatically determines which provider to use based on the model name prefix (e.g., "gpt-*" → OpenAI, "claude-*" → Anthropic) and only attempts to use providers that are actually configured.

### Success Criteria

- [ ] Service interface defined with `Context.Tag` following existing patterns
- [ ] Live layer implementation that uses `ConfigService` for provider configuration
- [ ] Automatically detects available providers from configured ones only
- [ ] Model selection comes from the method parameter, not environment variables
- [ ] Integration test that makes real API calls when `INTEGRATION_TEST=true`
- [ ] All tests pass and `pnpm run preflight` succeeds

## All Needed Context

### Documentation & References

```yaml
- url: https://effect-ts.github.io/effect/docs/ai/ai
  sections: ['Overview', 'Usage']
  why: Main documentation for @effect/ai package
  discovered_caveat: Use AiLanguageModel.make to create service implementations

- file: src/services/neo4j/Neo4j.service.ts
  why: Follow this exact pattern for service definition with Context.Tag
  gotcha: Service interface uses methods that return Effects

- file: src/layers/neo4j/Neo4j.layer.ts
  why: Pattern for creating Live layers that use ConfigService
  critical: Use Effect.scoped for resource management, provide test implementations

- file: src/services/config/index.ts
  why: Shows how ConfigService stores LLM provider configurations
  critical: providers are stored as Record<ProviderName, {apiKey, baseUrl, model}>

- file: src/domain/types/errors.ts
  why: Shows LlmApiError definition and error handling patterns
  critical: LlmApiError includes provider, statusCode, originalMessage

- file: src/domain/types/testCase.ts
  lines: 59-68
  why: Shows Message and Conversation types
  critical: Conversation is Chunk<Message>, Message has role and content

- file: docs/llms/guides/effect-packages/ai/ai/src/AiLanguageModel.ts
  why: Shows how to implement language model providers
  critical: Use AiLanguageModel.make with generateText and streamText methods

- file: docs/llms/guides/effect-packages/ai/anthropic/src/AnthropicLanguageModel.ts
  why: Shows Anthropic provider implementation pattern
  critical: Provider-specific configuration and layer setup

- file: docs/llms/guides/effect-packages/platform-node/examples/http-client.ts
  why: Pattern for making HTTP requests with @effect/platform
  critical: Shows HttpClient usage with filterStatusOk and error handling

- doc: https://www.npmjs.com/package/@effect/vitest
  section: ['Usage']
  why: Effect-specific vitest setup for testing
  critical: Use it.effect for Effect-based tests

- file: .env.example
  lines: 6-39
  why: Shows environment variable structure for LLM providers
  critical: Each provider needs API_KEY and BASE_URL variables (no MODEL needed)

- docfile: docs/llms/guides/effect-neo4j/README.md
  include_sections: ['Best Practices']
  why: General patterns for Effect services

- file: docs/llms/effect/effect-compliance-checklist.md
  why: Checklist to ensure Effect best practices are followed
  critical: Must complete all items before submitting PR
```

### Current Codebase tree

```bash
src/
├── domain/
│   └── types/
│       ├── errors.ts          # Contains LlmApiError
│       ├── testCase.ts        # Contains Message, Conversation types
│       └── branded.ts         # Contains LlmModel branded type
├── services/
│   ├── config/
│   │   └── index.ts          # ConfigService with LLM provider configs
│   └── neo4j/
│       └── Neo4j.service.ts  # Service pattern to follow
├── layers/
│   ├── configuration/
│   │   └── Configuration.layer.ts
│   └── neo4j/
│       └── Neo4j.layer.ts    # Layer pattern to follow
└── lib/
    └── test-utils.ts         # Test utilities
```

### Desired Codebase tree with files to be added

```bash
src/
├── services/
│   └── llm-api/
│       ├── index.ts              # Re-exports
│       ├── LlmApi.service.ts     # Service interface definition
│       └── LlmApi.integration.test.ts  # Integration tests
└── layers/
    └── llm-api/
        ├── index.ts              # Re-exports
        ├── LlmApi.layer.ts       # Live implementation
        ├── LlmApi.test-layers.ts # Test implementations
        └── LlmApi.test.ts        # Unit tests
```

### Known Gotchas

```typescript
// CRITICAL: @effect/ai requires specific provider packages
// Example: @effect/ai-openai, @effect/ai-anthropic, @effect/ai-google
// Example: Conversation is already Chunk<Message> type from testCase.ts
// Example: ConfigService only contains providers that are configured via env
// Example: Model is passed as parameter, not read from env variables
// Example: Integration tests only run when INTEGRATION_TEST=true
// Example: Each provider package has its own configuration pattern
// Example: Need to handle Redacted.value() to get actual API key values
// Example: Provider detection based on model prefix (gpt-*, claude-*, gemini-*)
```

## Implementation Blueprint

### Data models and structure

```typescript
// Service interface pattern from Neo4j.service.ts
export interface LlmApiImpl {
  readonly generate: (
    conversation: Conversation,
    model: string
  ) => Effect.Effect<string, LlmApiError, never>;
}

export class LlmApi extends Context.Tag('LlmApi')<
  LlmApi,
  LlmApiImpl
>() {}
```

### List of tasks

```yaml
Task 1: Update ConfigService to remove model requirement
MODIFY src/services/config/index.ts:
  - Remove LlmModel import
  - Remove model field from provider configuration type
  - Provider config should only have apiKey and baseUrl

MODIFY src/layers/configuration/Configuration.layer.ts:
  - Remove LlmModel import
  - Remove modelStr reading from loadProviderConfig
  - Remove model field from return object
  - Update ProviderConfig type to exclude model

MODIFY .env.example:
  - Remove all LLM_*_MODEL entries
  - Update comments to clarify model comes from test cases

Task 2: Create Service Definition
CREATE src/services/llm-api/LlmApi.service.ts:
  - MIRROR pattern from: src/services/neo4j/Neo4j.service.ts
  - Define LlmApiImpl interface with generate method
  - Create LlmApi service tag with Context.Tag
  - Import Conversation from domain/types/testCase
  - Import LlmApiError from domain/types/errors

CREATE src/services/llm-api/index.ts:
  - Export all from LlmApi.service.ts

Task 3: Create Layer Implementation
CREATE src/layers/llm-api/LlmApi.layer.ts:
  - MIRROR pattern from: src/layers/neo4j/Neo4j.layer.ts
  - Import @effect/ai and provider packages
  - Create make function that returns LlmApi service
  - Create LlmApiLive layer using ConfigService
  - Handle multiple providers (openai, anthropic, google)
  - Convert Conversation to provider message format
  - Use AiLanguageModel from @effect/ai

Task 4: Create Test Layers
CREATE src/layers/llm-api/LlmApi.test-layers.ts:
  - MIRROR pattern from: src/layers/neo4j/Neo4j.layer.ts test section
  - Create LlmApiTest with mock responses
  - Create LlmApiTestPartial using makeTestLayerFor

Task 5: Create Unit Tests
CREATE src/layers/llm-api/LlmApi.test.ts:
  - MIRROR pattern from: src/services/neo4j/Neo4j.test.ts
  - Test generate method with mock data
  - Test error handling scenarios
  - Use it.effect for Effect-based tests

Task 6: Create Integration Tests
CREATE src/services/llm-api/LlmApi.integration.test.ts:
  - Check INTEGRATION_TEST environment variable
  - Test real API calls to configured providers
  - Handle rate limits and timeouts appropriately
  - Skip tests if INTEGRATION_TEST !== 'true'

Task 7: Export from Index Files
CREATE src/layers/llm-api/index.ts:
  - Export LlmApiLive from LlmApi.layer.ts
  - Export test layers from LlmApi.test-layers.ts

Task 8: Install Dependencies
MODIFY package.json:
  - Add @effect/ai dependency
  - Add @effect/ai-openai dependency  
  - Add @effect/ai-anthropic dependency
  - Add @effect/ai-google dependency
  - Run pnpm install
```

### Per task pseudocode

```typescript
// Task 1: Update ConfigService
// src/services/config/index.ts
export class ConfigService extends Context.Tag('ConfigService')<
  ConfigService,
  {
    readonly neo4j: {
      readonly uri: Neo4jUri;
      readonly user: Neo4jUser;
      readonly password: Redacted.Redacted<string>;
    };
    readonly llm: {
      readonly providers: Record<
        ProviderName,
        {
          readonly apiKey: Redacted.Redacted<string>;
          readonly baseUrl: ApiBaseUrl;
          // REMOVED: readonly model: LlmModel;
        }
      >;
    };
  }
>() {}

// src/layers/configuration/Configuration.layer.ts
const loadProviderConfig = (providerName: string) =>
  Effect.gen(function* () {
    const prefix = `LLM_${providerName.toUpperCase()}`;
    
    const maybeApiKey = yield* Config.option(
      Config.redacted(`${prefix}_API_KEY`),
    );
    
    if (maybeApiKey._tag === 'None') {
      return null;
    }
    
    const baseUrlStr = yield* Config.string(`${prefix}_BASE_URL`);
    // REMOVED: const modelStr = yield* Config.string(`${prefix}_MODEL`);
    
    const baseUrl = yield* Schema.decode(ApiBaseUrl)(baseUrlStr);
    // REMOVED: const model = yield* Schema.decode(LlmModel)(modelStr);
    
    return {
      apiKey: maybeApiKey.value,
      baseUrl,
      // REMOVED: model,
    };
  });

// Task 2: Service Definition
import { Effect, Context } from 'effect';
import { Conversation } from '../../domain/types/testCase';
import { LlmApiError } from '../../domain/types/errors';

export interface LlmApiImpl {
  readonly generate: (
    conversation: Conversation,
    model: string
  ) => Effect.Effect<string, LlmApiError, never>;
}

export class LlmApi extends Context.Tag('LlmApi')<
  LlmApi,
  LlmApiImpl
>() {}

// Task 3: Layer Implementation (key parts)
import * as AiLanguageModel from '@effect/ai/AiLanguageModel';
import * as AiInput from '@effect/ai/AiInput';
import * as OpenAi from '@effect/ai-openai';
import * as Anthropic from '@effect/ai-anthropic';
import * as Google from '@effect/ai-google';
import { ConfigService } from '../../services/config';
import { Chunk, Redacted } from 'effect';

const make = (config: ConfigService['_A']) =>
  Effect.gen(function* () {
    // Get list of actually configured providers
    const configuredProviders = Object.keys(config.llm.providers);
    
    // Extract provider from model string and check if configured
    const getProvider = (model: string) =>
      Effect.gen(function* () {
        let providerName: string;
        
        if (model.startsWith('gpt')) {
          providerName = 'openai';
        } else if (model.startsWith('claude')) {
          providerName = 'anthropic';
        } else if (model.startsWith('gemini')) {
          providerName = 'google';
        } else {
          return yield* Effect.fail(new LlmApiError({
            provider: 'unknown',
            originalMessage: `Unknown model: ${model}`
          }));
        }
        
        // Check if this provider is actually configured
        if (!configuredProviders.includes(providerName)) {
          return yield* Effect.fail(new LlmApiError({
            provider: providerName,
            originalMessage: `Provider ${providerName} is not configured. Available providers: ${configuredProviders.join(', ')}`
          }));
        }
        
        return providerName;
      });

    // Convert Conversation to AI input format
    // Conversation is already Chunk<Message> from testCase.ts
    const convertToAiInput = (conversation: Conversation): AiInput.Raw => {
      return Chunk.toArray(conversation).map(msg => ({
        role: msg.role as 'user' | 'assistant' | 'system',
        content: msg.content
      }));
    };

    // Create provider-specific language model
    const createLanguageModel = (providerName: string, model: string) =>
      Effect.gen(function* () {
        // Provider is guaranteed to exist at this point
        const providerConfig = config.llm.providers[providerName];
        
        const apiKey = Redacted.value(providerConfig.apiKey);
        const baseUrl = providerConfig.baseUrl;
        
        switch (providerName) {
          case 'openai':
            return yield* OpenAi.OpenAiLanguageModel.fromDefaults({
              apiKey,
              baseURL: baseUrl,
              model // Use the specific model passed in, not from config
            });
          case 'anthropic':
            return yield* Anthropic.AnthropicLanguageModel.fromDefaults({
              apiKey,
              baseURL: baseUrl,
              model // Use the specific model passed in, not from config
            });
          case 'google':
            return yield* Google.GoogleLanguageModel.fromDefaults({
              apiKey,
              baseURL: baseUrl,
              model // Use the specific model passed in, not from config
            });
          default:
            return yield* Effect.fail(new LlmApiError({
              provider: providerName,
              originalMessage: `Unsupported provider: ${providerName}`
            }));
        }
      });

    const generate = (conversation: Conversation, model: string) =>
      Effect.gen(function* () {
        const provider = yield* getProvider(model);
        const languageModel = yield* createLanguageModel(provider, model);
        const messages = convertToAiInput(conversation);
        
        const response = yield* pipe(
          AiLanguageModel.generateText({
            prompt: messages,
            system: undefined // Could extract system message if needed
          }),
          Effect.provideService(AiLanguageModel.AiLanguageModel, languageModel),
          Effect.mapError(error => new LlmApiError({
            provider,
            originalMessage: error.message,
            statusCode: 'statusCode' in error ? error.statusCode : undefined
          }))
        );
        
        // Extract text from response parts
        return response.parts
          .filter(part => part._tag === 'TextPart')
          .map(part => part.text)
          .join('');
      });

    return LlmApi.of({ generate });
  });

export const LlmApiLive = Layer.effect(
  LlmApi,
  Effect.gen(function* () {
    const config = yield* ConfigService;
    return yield* make(config);
  })
);

// Task 4: Test Layer
import { makeTestLayerFor } from '../../lib/test-utils';

export const LlmApiTest = (mockData: Map<string, string> = new Map()) =>
  Layer.succeed(
    LlmApi,
    LlmApi.of({
      generate: (conversation, model) =>
        Effect.gen(function* () {
          const key = `${Chunk.toArray(conversation).map(m => m.content).join(':')}:${model}`;
          const response = mockData.get(key) || `Mock response for ${model}`;
          yield* Effect.logDebug(`Mock LLM API: ${key} -> ${response}`);
          return response;
        })
    })
  );

export const LlmApiTestPartial = (impl: Partial<LlmApiImpl>) =>
  makeTestLayerFor(LlmApi)(impl);

// Task 6: Integration Test pattern
import { describe, it, expect } from 'vitest';
import { Effect, pipe } from 'effect';

describe.skipIf(process.env.INTEGRATION_TEST !== 'true')(
  'LlmApi Integration Tests',
  () => {
    it.effect('should generate text using real API', () =>
      Effect.gen(function* () {
        const llmApi = yield* LlmApi;
        const conversation = Chunk.of(
          { role: 'user' as const, content: 'Say hello in one word' }
        );
        
        const result = yield* llmApi.generate(conversation, 'gpt-3.5-turbo');
        expect(result.toLowerCase()).toContain('hello');
      }).pipe(
        Effect.provide(LlmApiLive),
        Effect.provide(ConfigServiceLive),
        Effect.timeout('30 seconds')
      )
    );
  }
);
```

### Integration Points

```yaml
CONFIG:
  - uses: ConfigService.llm.providers
  - pattern: "Record<ProviderName, { apiKey, baseUrl }>"
  - note: API keys are Redacted, use Redacted.value() to access
  - note: Only contains providers that are actually configured
  - note: No model field - model comes from method parameter
  
ENVIRONMENT:
  - variables: LLM_OPENAI_API_KEY, LLM_OPENAI_BASE_URL, etc.
  - test flag: INTEGRATION_TEST=true
  - providers: Set via LLM_PROVIDERS env var or config/llm-providers.txt
  - detection: Service only uses providers that exist in config
  
DEPENDENCIES:
  - @effect/ai (core package)
  - @effect/ai-openai (OpenAI provider)
  - @effect/ai-anthropic (Anthropic provider)
  - @effect/ai-google (Google provider)
  
TYPES:
  - Conversation from src/domain/types/testCase (Chunk<Message>)
  - Message from src/domain/types/testCase (has role and content)
  - LlmApiError from src/domain/types/errors
  - ContentRole from src/domain/types/contentNode
```

## Validation Loop

### Level 1: Syntax & Type Checking

```bash
# Install dependencies first
pnpm add @effect/ai @effect/ai-openai @effect/ai-anthropic @effect/ai-google

# Run these FIRST - fix any errors before proceeding
pnpm run build                    # TypeScript compilation
pnpm run lint                     # ESLint checking

# Expected: No errors. If errors, READ the error and fix.
```

### Level 2: Unit Tests

```typescript
// Test cases to implement in LlmApi.test.ts
it.effect('should generate response for valid conversation', () =>
  Effect.gen(function* () {
    const mockResponse = "Hello! How can I help you?";
    const testData = new Map([
      ['Hello:gpt-4', mockResponse]
    ]);
    const testLayer = LlmApiTest(testData);
    
    const llmApi = yield* LlmApi;
    const result = yield* llmApi.generate(
      Chunk.of({ role: 'user', content: 'Hello' }),
      'gpt-4'
    );
    expect(result).toBe(mockResponse);
  }).pipe(Effect.provide(testLayer))
);

it.effect('should handle provider errors', () =>
  Effect.gen(function* () {
    const testLayer = LlmApiTestPartial({
      generate: () => Effect.fail(new LlmApiError({
        provider: 'test',
        statusCode: 429,
        originalMessage: 'Rate limit exceeded'
      }))
    });
    
    const llmApi = yield* LlmApi;
    const exit = yield* Effect.exit(
      llmApi.generate(Chunk.empty(), 'test-model')
    );
    expect(exit._tag).toBe('Failure');
  }).pipe(Effect.provide(testLayer))
);

it.effect('should handle unknown models', () =>
  Effect.gen(function* () {
    const llmApi = yield* LlmApi;
    const exit = yield* Effect.exit(
      llmApi.generate(
        Chunk.of({ role: 'user', content: 'test' }),
        'unknown-model'
      )
    );
    expect(exit._tag).toBe('Failure');
    if (exit._tag === 'Failure') {
      const error = exit.cause._tag === 'Fail' ? exit.cause.error : null;
      expect(error).toBeInstanceOf(LlmApiError);
      expect(error?.originalMessage).toContain('Unknown model');
    }
  }).pipe(
    Effect.provide(LlmApiLive),
    Effect.provide(ConfigServiceLive)
  )
);

it.effect('should fail when provider not configured', () =>
  Effect.gen(function* () {
    // Assume only openai is configured in test
    const llmApi = yield* LlmApi;
    const exit = yield* Effect.exit(
      llmApi.generate(
        Chunk.of({ role: 'user', content: 'test' }),
        'claude-3-opus' // Anthropic model but not configured
      )
    );
    expect(exit._tag).toBe('Failure');
    if (exit._tag === 'Failure') {
      const error = exit.cause._tag === 'Fail' ? exit.cause.error : null;
      expect(error).toBeInstanceOf(LlmApiError);
      expect(error?.originalMessage).toContain('not configured');
    }
  }).pipe(
    Effect.provide(LlmApiLive),
    Effect.provide(ConfigServiceLive)
  )
);
```

```bash
# Run and iterate until passing:
pnpm test LlmApi.test.ts
```

### Level 3: Integration Test

```bash
# Set up environment
export INTEGRATION_TEST=true
export LLM_PROVIDERS=openai,anthropic
export LLM_OPENAI_API_KEY=your-key
export LLM_OPENAI_BASE_URL=https://api.openai.com/v1
# Note: No LLM_OPENAI_MODEL - model comes from test case
export LLM_ANTHROPIC_API_KEY=your-key
export LLM_ANTHROPIC_BASE_URL=https://api.anthropic.com/v1
# Note: No LLM_ANTHROPIC_MODEL - model comes from test case

# Run integration tests
pnpm test LlmApi.integration.test.ts

# Expected: Tests pass with real API responses
# If error: Check API keys, network, rate limits
```

### Level 4: Effect Compliance Checklist

```bash
# Review against docs/llms/effect/effect-compliance-checklist.md
# Ensure all items are completed:
- [ ] Using Effect.gen with function* syntax
- [ ] Proper error handling with typed errors
- [ ] Resource management with Effect.scoped where needed
- [ ] Following functional programming principles
- [ ] No mutations, using immutable data structures
- [ ] Proper service/layer separation
```

## Final Validation Checklist

- [ ] All tests pass: `pnpm test`
- [ ] No linting errors: `pnpm run lint`
- [ ] No type errors: `pnpm run build`
- [ ] Preflight passes: `pnpm run preflight`
- [ ] Integration test successful with real API
- [ ] Error cases handled with LlmApiError
- [ ] Service follows existing patterns exactly
- [ ] Complete effect compliance checklist items
- [ ] Dependencies added to package.json
- [ ] Documentation clear for setup

## Anti-Patterns to Avoid

- ❌ Don't use Promise/async-await - use Effect
- ❌ Don't hardcode API keys - use Config service
- ❌ Don't catch all errors - use typed LlmApiError
- ❌ Don't skip provider validation - check config exists
- ❌ Don't ignore rate limits - handle 429 errors
- ❌ Don't create new patterns - follow Neo4j service exactly
- ❌ Don't use native fetch - use @effect/ai abstractions
- ❌ Don't forget to handle Redacted values from Config
- ❌ Don't mix concerns - keep provider logic separate
- ❌ Don't skip Effect compliance checklist

---

**Confidence Score: 9/10**

This PRP provides comprehensive context for implementing the LLM API Service using the @effect/ai package. The score is 9/10 because:
- Clear patterns from existing services (Neo4j) to follow
- Specific file locations and examples provided
- Integration test approach defined
- Effect-ai package documentation and examples included
- Clear separation of concerns with provider-specific handling
- Properly uses existing Conversation and Message types
- Correctly detects only configured providers
- Model selection from parameter, not environment
- The only uncertainty is around exact provider package APIs which may require minor adjustments during implementation